{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN的可解释性.ipynb","provenance":[],"collapsed_sections":["_q9QOhPFVlJP","4Kml4vtiV8A4","kYbAKD7QPTLZ"],"mount_file_id":"1xnVv1kAx_I6yUl3Gz3_PsGinGjOIdnfY","authorship_tag":"ABX9TyNlPPwZvcj4BR+1VH4kqx7e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XiGMSUztB9aC"},"source":["!pip install lime==0.1.1.37\n","!gdown --id '19CzXudqN58R3D-1G8KeFWk8UDQwlb8is' --output food-11.zip # 下載資料集\n","!unzip food-11.zip # 解壓縮"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_q9QOhPFVlJP"},"source":["# train一个基础分类任务的CNN"]},{"cell_type":"code","metadata":{"id":"rQ7Gw0BcDG46","executionInfo":{"status":"ok","timestamp":1621824055247,"user_tz":-480,"elapsed":347,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}}},"source":["# make labels\n","import os\n","def EarnName():\n","    path = \"/content/food-11/\"\n","    original_images = []\n","    pict_name = open('train.txt', 'w+')\n","    for root, dirs, filenames in os.walk(path):\n","        for filename in filenames:\n","            original_images.append(root + \"/\" + filename)\n","    original_images = sorted(original_images)\n","    print('num: {}'.format(len(original_images)))\n","    for filename in (original_images):\n","        filename = filename.replace('\\\\', '/')\n","        print(filename)\n","        pict_name.write(filename + '\\n')\n","    pict_name.close()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7MPTGVeF67r","executionInfo":{"status":"ok","timestamp":1621825215196,"user_tz":-480,"elapsed":4817,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}}},"source":["# dataset: make dataset\n","import os\n","import torch\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","import cv2\n","import numpy as np\n","from PIL import Image"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"5syrmt4mVh-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621819086267,"user_tz":-480,"elapsed":2556,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}},"outputId":"88b7157d-a6dc-473f-f318-d3e973ee77bb"},"source":["TRAIN_PATH = '/content/drive/MyDrive/Lee/food11_train.csv'\n","TEST_PATH = '/content/drive/MyDrive/Lee/food11_val.csv'\n","BATCH_SIZE = 128\n","IMG_SIZE = 140\n","CROP_SIZE = 128\n","MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","BEAT_ACC = 0.\n","SAVE_PATH = '/content/drive/MyDrive/Lee/weights/best.pth.tar'\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomChoice([\n","        transforms.Resize((IMG_SIZE, IMG_SIZE)),                \n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(15),\n","        transforms.RandomCrop((CROP_SIZE, CROP_SIZE)),\n","        transforms.RandomOrder([\n","            transforms.ColorJitter(brightness=0.1),\n","            transforms.ColorJitter(saturation=0.2),\n","            transforms.ColorJitter(contrast=0.2),\n","        ]),\n","    ]),\n","    transforms.Resize((CROP_SIZE, CROP_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(MEAN, STD),\n","])\n","test_transform = transforms.Compose([\n","    transforms.Resize((CROP_SIZE, CROP_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(MEAN, STD),\n","])\n","\n","class ImgDataset(Dataset):\n","    def __init__(self, imgs_path, imgs_label, is_training=True):\n","        self.imgs_path = imgs_path\n","        self.imgs_label = imgs_label\n","        self.is_training = is_training\n","\n","    def __getitem__(self, index):\n","        img_path = self.imgs_path[index]\n","        img_label = self.imgs_label[index]\n","        img = Image.open(os.path.join('/content', img_path)).convert('RGB')\n","        if self.is_training:\n","            img = train_transform(img)\n","        else:\n","            img = test_transform(img)\n","        return img, img_label\n","\n","    def __len__(self):\n","        return len(self.imgs_path)\n","\n","train_info = pd.read_csv(TRAIN_PATH)\n","train_imgs = train_info['name']\n","train_labels = train_info['label']\n","test_info = pd.read_csv(TEST_PATH)\n","test_imgs = test_info['name']\n","test_labels = test_info['label']\n","\n","trainDS = ImgDataset(train_imgs, train_labels, is_training=True)\n","trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle=True, drop_last=True)\n","testDS = ImgDataset(test_imgs, test_labels, is_training=False)\n","testDL = DataLoader(testDS, BATCH_SIZE, shuffle=False)\n","\n","for idx, (imgs, labels) in enumerate(trainDL):\n","    print(imgs.shape)\n","    print(labels)\n","    break "],"execution_count":4,"outputs":[{"output_type":"stream","text":["torch.Size([128, 3, 128, 128])\n","tensor([ 5,  2,  9,  4, 10,  4,  5,  5,  9,  5,  9,  6,  8,  6,  2,  4,  5,  1,\n","         5,  5,  0,  9,  6,  2,  3,  5,  2,  1,  4,  8,  5,  3,  5,  1,  5,  2,\n","         7,  2,  5,  8,  3,  8,  2,  1,  3,  9,  3, 10,  3,  5,  2,  7, 10, 10,\n","         5,  2,  0,  9,  2,  0,  9,  2,  2,  0,  9,  2,  3, 10,  8,  0,  8,  9,\n","         5,  2,  6,  0,  9,  4,  2, 10,  9,  2,  5,  9,  9,  9,  9, 10,  3,  9,\n","         9,  8,  9,  3,  5,  5, 10,  5,  9,  9,  9,  0, 10,  9,  5,  7,  9,  5,\n","         9,  7,  5,  0, 10,  5,  2,  8,  9,  2, 10,  2,  5,  9, 10,  1,  1,  8,\n","         2,  8])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LZl-KUFkcXEp","executionInfo":{"status":"ok","timestamp":1621819099549,"user_tz":-480,"elapsed":11172,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}}},"source":["import torch.nn as nn\n","class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 64, 3, 1, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(64, 128, 3, 1, 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(128, 256, 3, 1, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(256, 512, 3, 1, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(512, 512, 3, 1, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(512*4*4, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 11)\n","        )\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)\n","\n","model = Classifier().cuda()\n","loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n","EPOCH = 100\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rD0ps_6jkgQ","executionInfo":{"status":"ok","timestamp":1621819106526,"user_tz":-480,"elapsed":306,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}}},"source":["import math\n","from torch.optim.lr_scheduler import LambdaLR\n","def Cos_warmup(optimizer, epoch_warmup, epoch_training, num_cycles=0.5, last_epoch=-1):\n","    def lr_lambda(current_epoch):\n","        if current_epoch < epoch_warmup:\n","            return float(current_epoch)/float(max(1, epoch_warmup))\n","\n","        process = float(current_epoch-epoch_warmup)/\\\n","                  float(max(1, epoch_training-epoch_warmup))\n","        return max(0.0, 0.5*(1.0+math.cos(math.pi*float(num_cycles)*2.0*process)))\n","    return LambdaLR(optimizer, lr_lambda, last_epoch)\n","\n","\n","cosWarmUp = Cos_warmup(\n","    optimizer,\n","    epoch_warmup=5,\n","    epoch_training=100\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeOW1kzZqxEt","executionInfo":{"status":"ok","timestamp":1621819108635,"user_tz":-480,"elapsed":383,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}}},"source":["x = torch.randn((2, 3, 128, 128))\n","y=model(x.cuda())"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5eEEP1EedUE","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1621821713684,"user_tz":-480,"elapsed":494,"user":{"displayName":"yan Yan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCTMyxCywLUps0ALJLCuUiPqF_ZNUn_in9o05WYw=s64","userId":"15231441231973556955"}},"outputId":"c002ad79-594b-430e-d784-5a53565cf01c"},"source":["import time\n","\n","for epoch in range(EPOCH):\n","    epoch_start_time = time.time()\n","    train_acc = 0.\n","    train_loss = 0.\n","    val_acc = 0.\n","    val_loss = 0.\n","\n","    model.train()\n","    for i, (imgs, labels) in enumerate(trainDL):\n","        optimizer.zero_grad()\n","        train_pred = model(imgs.cuda())\n","        batch_loss = loss(train_pred, labels.cuda())\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == labels.numpy())\n","        train_loss += batch_loss.item()\n","    cosWarmUp.step()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (imgs, labels) in enumerate(testDL):\n","            val_pred = model(imgs.cuda())\n","            batch_loss = loss(val_pred, labels.cuda())\n","            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == labels.numpy())\n","            val_loss += batch_loss.item()\n","        #將結果 print 出來\n","        if (val_acc/testDS.__len__()) > BEAT_ACC:\n","            state = {\n","                'epoch': epoch,\n","                'model': model.state_dict()\n","            }\n","            torch.save(state, SAVE_PATH)\n","            print('saving model to:'+ SAVE_PATH)\n","            BEAT_ACC = (val_acc/testDS.__len__())\n","        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n","            (epoch + 1, EPOCH, time.time()-epoch_start_time, \\\n","             train_acc/trainDS.__len__(), train_loss/trainDS.__len__(), val_acc/testDS.__len__(), val_loss/testDS.__len__()))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dd0ea1bff410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'EPOCH' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"CCMKUJo5VvrX"},"source":["# CNN的可解释性"]},{"cell_type":"markdown","metadata":{"id":"4Kml4vtiV8A4"},"source":["# saliency map\n","\n","原理：\n","\n","---\n","在我们计算loss的时候，与loss相关的项有image，label，model\n","parameter。因此loss的计算backward时我们只在乎loss对model parameter的偏微分\n","值，在数学上image本身也是continuous tensor，我们可以计算loss对image的偏微分值，而这个值代表了在参数和标签都固定的情况下，稍微改变image的pixel value对loss的影响是什么，人们把这种变化的剧烈程度解读为pixel的重要性（每个pixel都有自己的偏微分值），因此吧同一张图中loss对每个pixel的偏微分值计算出来就可以知道那些位置是model在判断时的重要依据。\n","\n","操作：\n","\n","---\n","一般情况下在forward后算出loss，然后进行backward，而这个backward，pytorch预设是计算loss对参数的偏微分值，只需要告诉pytorch，image也是要算的偏微分对象之一。"]},{"cell_type":"code","metadata":{"id":"9oqhr77jpkZq"},"source":["def normalize(image):\n","    return (image - image.min()) / (image.max() - image.min())\n","\n","def compute_saliency_maps(x, y, model):\n","    model.eval()\n","    x = x.cuda()\n","\n","    x.requires_grad_()\n","    y_pred = model(x)\n","    loss_func = torch.nn.CrossEntroyLoss()\n","    loss = loss_func(y_pred, y.cuda())\n","    loss.backward()\n","\n","    saliencies = x.grad.abs().detach().cpu()\n","    saliencies = torch.stack([normalize(item) for item in saliencies])\n","    return saliencies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MPci3xuqyXK"},"source":["img_indices = [83, 4218, 4707, 8598]\n","images, labels = train_set.getbatch(img_indices)\n","saliencies = compute_saliency_maps(images, labels, model)\n","\n","fig, axs = plt.subplots(2, len(img_indices) figsize=(15, 8))\n","for row, target in enumerate([images, saliencices]):\n","    for coloumn, img in enumerate(target):\n","        axs[row][colunmn].imshow(img.permute(1, 2, 0).numpy())\n","\n","plt.show()\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYbAKD7QPTLZ"},"source":["# Filter explaination\n","原理：\n","\n","---\n","希望知道某一个filter到底认出了什么，有两件事可以做：\n","\n","filer activation: 挑选几张图片，看看图片中那些位置会activate这个filter\n","\n","filter visualization： 怎样的image可以最大程度的activate这个filter\n","\n","操作：\n","\n","---\n","在操作层面的原理上可以直接修改model definition，让forward不止返回loss，还有activation map，但在实际上是使用pytorch中的hook函数\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kKyKvhn-Q46y"},"source":["def normalize(image):\n","    return (image-image.min()) / (image.max() - image.min())\n","\n","layer_activations = None\n","\n","def filter_explaination(x, model, cnnid, filterid, iteration=100, lr=1):\n","    \"\"\"\n","    x:对于被指定的图片，那些位置可以activate被指定的filter\n","    \"\"\"\n","    model.eval()\n","\n","    def hook(model, input, output):\n","        global layer_activations\n","        layer_activations = output\n","\n","    hook_handle = model.children[cnnid].register_forward_hook(hook)\n","\n","    # filter activation：x经过被指定filter的activation map\n","    model(x.cuda())\n","    filter_activations = layer_activations[:, filterid, :, :].detach().cpu()\n","\n","    # filter visualization：找出可以最大程度activate这个filter的图片\n","    x = x.cuda()\n","    x.requires_grad_()\n","    optimizer = torch.optim.Adam([x], lr=lr)\n","\n","    for iter in range(iteration):\n","        optimizer.zero_grad()\n","        model(x)\n","\n","        objective = -layer_activations[:, filterid, :, :].sum()\n","\n","        objective.backward()\n","        optimizer.step()\n","    filter_visualization = x.detach().cpu().squeeze()[0]\n","    hook_handle.remove()\n","\n","    return filter_activations, filter_visualization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UC09zkkaPpS"},"source":["img_indices = [83, 4218, 4707, 8598]\n","images, labels = train_set.getbatch(img_indices)\n","filter_activations, filter_visualization = filter_explaination(images, model, cnnid=15, filterid=0, iteration=100, lr=0.1)\n","\n","plt.imshow(normalize(filter_visualization.permute(1, 2, 0)))\n","plt.show()\n","plt.close()\n","\n","fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n","for i, img in enumerate(images):\n","    axs[0][1].imshow(img.permute(1, 2, 0))\n","for i, img in enumerate(filter_activations):\n","    axs[1][1].imshow(normalize(img))\n","plt.show()\n","plt.imshow()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emz_jYaTlnzZ"},"source":["# Lime\n","\n","基于现成的库"]},{"cell_type":"code","metadata":{"id":"u2yY3X0qlx7h"},"source":["from skimage.segmentation import slic\n","from lime import lime_image\n","from pdb import set_trace\n","def predict(input):\n","    model.eval()\n","    input = torch.FloatTensor(input).permute(0, 3, 1, 2)\n","    \n","    output = model(input.cuda())\n","    return output.detach().cpu().numpy()\n","\n","def segmentation(input):\n","    return slic(input, n_segments=100, compactness=1, sigma=1)\n","\n","img_indices = [83, 4218, 4707, 8598]\n","images, labels = train_set.getbatch(img_indices)\n","fig, axs = plt.subplots(1, 4, figsize=(15, 8))\n","np.random.seed(16)\n","\n","for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):\n","    x = image.astype(np.double)\n","    explainer = lime_image.LimeImageExplainer()\n","    explaination = explainer.explain_instance(\n","        image=x, \n","        classifier_fn=predict, # 定义图片如何经过model得到prediction\n","        segmentation_fn=segmentation  # 定义如何把图片做segmentatin\n","    )\n","\n","    lime_img, mask = explaination.get_image_and_mask(\n","        label=label.item(),\n","        positive_only=False,\n","        hide_rest=False,\n","        num_features=11,\n","        min_weight=0.05\n","    )\n","\n","    axs[idx].inshow(lime_img)\n","\n","plt.show()\n","plt.close()"],"execution_count":null,"outputs":[]}]}